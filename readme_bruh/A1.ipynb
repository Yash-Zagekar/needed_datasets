{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment - 01\n",
        "Write a program for pre-processing of a text document such as stop\n",
        "word removal, stemming."
      ],
      "metadata": {
        "id": "5UyS9HZutS3Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing required Libraries\n",
        "\n"
      ],
      "metadata": {
        "id": "-BTSklXqtwYH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9tn5dcFBs50a",
        "outputId": "bde77f36-bcb8-4630-d331-cbbd0f0026db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.6)\n"
          ]
        }
      ],
      "source": [
        "# NLTK - Stands for Natural Language Toolkit, a popular Python library used in natural language processing (NLP).\n",
        "# NLTK provides tools to work with human language data, making it easier to perform tasks like text analysis,\n",
        "# tokenization, stemming, and sentiment analysis.\n",
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " # Import Libraries and Download NLTK Resources"
      ],
      "metadata": {
        "id": "7oj03NcZukCM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "# collection of words like ('The','is','in') don't add much effect. These are called \"stopwords,\" and theyâ€™re often removed to focus on the more important words in a text.\n",
        "from nltk.corpus import stopwords\n",
        "# Tokenize, means to divide sentence into individual words. \"waga no nama yhwach\" : tokenize : [\"waga\", \"no\", \"nama\", \"yhwach\"]\n",
        "from nltk.tokenize import word_tokenize\n",
        "# PorterStemmer - To reduce word to there stem form - Runner - run , Playing - play\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "# Download required NLTK resources\n",
        "nltk.download('punkt') # Punkt Sentence Tokenizer (required for tokenization)\n",
        "nltk.download('stopwords') # List of stop words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Zk0WOcIugko",
        "outputId": "d34753a8-f42f-4282-8bd7-d6b2352d23ce"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define the Preprocessing Function"
      ],
      "metadata": {
        "id": "7F8em_Sawt-u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text):\n",
        "    # Converted into lower case\n",
        "    text = text.lower()\n",
        "\n",
        "    # Converted text into individual words (tokenize)\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Removed punctuation\n",
        "    tokens = [word for word in tokens if word.isalnum()]\n",
        "\n",
        "    # Stop word remover\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "    # Initialize stemmer\n",
        "    stemmer = PorterStemmer()\n",
        "\n",
        "    # Apply stemming\n",
        "    stemmed_tokens = [stemmer.stem(word) for word in filtered_tokens]\n",
        "\n",
        "    preprocess = \" \".join(stemmed_tokens)\n",
        "\n",
        "    return preprocess"
      ],
      "metadata": {
        "id": "Ap2-zlL8w4ws"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example text input\n",
        "text_input = \"sample.txt\"\n",
        "with open(text_input, 'r', encoding='utf-8') as file :\n",
        "    text = file.read()\n",
        "# Preprocess the text\n",
        "processed_text = preprocess_text(text)\n",
        "\n",
        "# Output the processed tokens\n",
        "print(processed_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hrgg4Zpvx17R",
        "outputId": "9402fd62-3160-47a0-c8dd-dbd33a6e3395"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "attent need groundbreak research paper publish 2017 vaswani et al introduc transform architectur revolution natur languag process nlp paper propos model reli entir mechan elimin need recurr convolut layer architectur consist structur encod process input sequenc decod gener output sequenc mechan allow model weigh import differ word sentenc rel one anoth captur depend effect one key innov attent enabl model focu variou aspect input simultan enhanc abil understand complex relationship within data transform parallel capabl allow effici train larg dataset significantli speed learn process compar tradit recurr neural network sinc introduct transform becom foundat numer nlp model includ bert gpt wide adopt applic machin translat text summar languag gener work fundament chang approach sequenc model nlp lead remark advanc field\n"
          ]
        }
      ]
    }
  ]
}